# -*- coding: utf-8 -*-
"""image_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m4auYJolFR7lqyTZRx5YDU6H0pv_COpH
"""

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow import keras
from tqdm import tqdm

device = "/gpu:0" if tf.config.list_physical_devices("GPU") else "/cpu:0"


data_dir = '/content/drive/MyDrive/train'
num_classes = 102
image_size = (224, 224)
learning_rate = 0.001
batch_size = 64
num_epochs = 10


base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')
base_model.trainable = False


classifier = models.Sequential([
    layers.GlobalAveragePooling2D(),
    layers.Dense(num_classes, activation='softmax')
])


model = models.Sequential([
    base_model,
    classifier
])


data_augmentation = keras.Sequential([
    layers.experimental.preprocessing.Rescaling(1./255),
    layers.experimental.preprocessing.RandomCrop(224, 224),
    layers.experimental.preprocessing.RandomFlip("horizontal"),
])



train_dataset = image_dataset_from_directory(
    data_dir,
    labels='inferred',
    label_mode='int',
    batch_size=batch_size,
    image_size=image_size,
    shuffle=True,
    validation_split=0.2,
    subset='training',
    seed=123
)

val_dataset = image_dataset_from_directory(
    data_dir,
    labels='inferred',
    label_mode='int',
    batch_size=batch_size,
    image_size=image_size,
    shuffle=True,
    validation_split=0.2,
    subset='validation',
    seed=123
)


train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


criterion = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = optimizers.Adam(learning_rate=learning_rate)


results_list = []

for epoch in range(num_epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in tqdm(train_dataset, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch'):
        inputs, labels = inputs, labels

        with tf.device(device):
            with tf.GradientTape() as tape:
                outputs = model(inputs)
                loss = criterion(labels, outputs)

            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            running_loss += loss.numpy()

            predicted = tf.argmax(outputs, axis=1)
            predicted = tf.cast(predicted, tf.int32)
            labels = tf.cast(labels, tf.int32)
            correct += tf.reduce_sum(tf.cast(tf.equal(predicted, labels), tf.float32))

            total += labels.shape[0]

    epoch_loss = running_loss / len(train_dataset)
    accuracy = 100 * correct / total

    results_list.append({'Epoch': epoch + 1, 'Loss': epoch_loss, 'Accuracy': accuracy})


for result in results_list:
    print(f"Epoch {result['Epoch']}/{num_epochs} - Loss: {result['Loss']:.4f}, Accuracy: {result['Accuracy']:.2f}%")

import os

data_dir = '/content/drive/MyDrive/train'


class_names = {}
class_folders = os.listdir(data_dir)


class_folders.sort()


for i, folder in enumerate(class_folders):
    class_names[i] = folder


print(class_names)

from tensorflow.keras.preprocessing import image
import numpy as np

img_path = '/content/drive/MyDrive/train/25/image_06575.jpg'
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = img_array / 255.0

predictions = model.predict(img_array)
predicted_class = np.argmax(predictions, axis=1)[0]


print("Predicted Class Index:", predicted_class)